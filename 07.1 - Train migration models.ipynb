{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os, time, math, csv\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.optimize\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import MigrationModels\n",
    "import MigrationEvaluationMethods\n",
    "\n",
    "#simple-maps\n",
    "sys.path.append(os.path.join(os.getcwd(),\"simple-maps\"))\n",
    "from simplemaps.SimpleFigures import simpleMap, differenceMap, simpleBinnedMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, AlphaDropout, Input, BatchNormalization, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2004, 2014 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_fn = \"data/intermediate/boundary_shapefiles/cb_2015_us_county_500k.shp\"\n",
    "shapefile_key = \"GEOID\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_matrices = [\n",
    "    np.load(\"data/processed/migration/migration_matrix_%d.npy\" % (year))\n",
    "    for year in years\n",
    "]\n",
    "\n",
    "for migration_matrix in migration_matrices:\n",
    "    np.fill_diagonal(migration_matrix, 0.0)\n",
    "\n",
    "f = open(\"data/processed/county_intersection_list_2004_2014.txt\")\n",
    "county_list = f.read().strip().split(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "county_fips_to_idx = {fips:idx for idx, fips in enumerate(county_list)}\n",
    "num_counties = len(county_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/processed/hurricane_affected_counties.csv\", \"r\")\n",
    "flooded_counties = f.read().strip().split(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "unflooded_counties = [fips for fips in county_list if fips not in flooded_counties]\n",
    "\n",
    "flooded_county_idxs = np.array([county_fips_to_idx[fips] for fips in flooded_counties])\n",
    "unflooded_county_idxs = np.array([county_fips_to_idx[fips] for fips in unflooded_counties])\n",
    "\n",
    "all_county_idxs = np.array(list(range(num_counties)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per county features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_by_year = pd.read_csv(\"data/processed/county_population_2004_2014.csv\", dtype={\"FIPS\":str})\n",
    "population_by_year = population_by_year.set_index(\"FIPS\")\n",
    "\n",
    "population_vectors_by_year = [population_by_year[\"POPESTIMATE%d\" % (year)].values.reshape(-1,1) for year in years]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.load(\"data/processed/county_distance_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervening_population_opportunities = []\n",
    "for i in range(len(years)):\n",
    "    s = MigrationModels.getInterveningOpportunities(population_vectors_by_year[i], distances)\n",
    "    intervening_population_opportunities.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_dataset(year_idx, origin_list, destination_list, model=None):\n",
    "\n",
    "    origin_pop = population_vectors_by_year[year_idx][origin_list,:].astype(float)\n",
    "    destination_pop = population_vectors_by_year[year_idx][destination_list,:].astype(float)\n",
    "    S = intervening_population_opportunities[year_idx][origin_list,:][:,destination_list].astype(float)\n",
    "    D = distances[origin_list,:][:,destination_list].astype(float)\n",
    "    T = migration_matrices[year_idx][origin_list,:][:,destination_list].astype(float)\n",
    "\n",
    "    t_model = LinearRegression(fit_intercept=False)\n",
    "    t_model.fit(origin_pop, T.sum(axis=1))\n",
    "    beta = t_model.coef_[0]\n",
    "\n",
    "    args = {\n",
    "        \"origin_pop\": origin_pop,\n",
    "        \"destination_pop\": destination_pop,\n",
    "        \"S\": S,\n",
    "        \"D\": D,\n",
    "        \"T\": T,\n",
    "        \"beta\": beta,\n",
    "        \"model\": model\n",
    "    }\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs_from_full_dataset(args):\n",
    "    \n",
    "    origin_pop = args[\"origin_pop\"]\n",
    "    destination_pop = args[\"destination_pop\"]\n",
    "    S = args[\"S\"]\n",
    "    D = args[\"D\"]\n",
    "    T = args[\"T\"]\n",
    "    \n",
    "    num_rows, num_cols = origin_pop.shape[0], destination_pop.shape[0]\n",
    "    num_entries = num_rows * num_cols\n",
    "    num_features = 4\n",
    "    \n",
    "    X = np.zeros((num_entries, num_features), dtype=float)\n",
    "    Y = np.zeros(num_entries, dtype=float)    \n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            \n",
    "            idx = (i*num_cols) + j\n",
    "            \n",
    "            origin_idx = i\n",
    "            dest_idx = j\n",
    "            \n",
    "            X[idx,:] = [\n",
    "                origin_pop[origin_idx],\n",
    "                destination_pop[dest_idx],\n",
    "                D[origin_idx, dest_idx],\n",
    "                S[origin_idx, dest_idx],\n",
    "            ]\n",
    "            \n",
    "            Y[idx] = T[origin_idx, dest_idx]\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_traditional_models(alpha, args):\n",
    "    model = args[\"model\"]\n",
    "    origin_pop, destination_pop = args[\"origin_pop\"], args[\"destination_pop\"]\n",
    "    S, D, T = args[\"S\"], args[\"D\"], args[\"T\"]\n",
    "    beta = args[\"beta\"]\n",
    "    \n",
    "    if model == \"extrad\":\n",
    "        P = MigrationModels.extendedRadiationModel(origin_pop, destination_pop, S, alpha)\n",
    "    elif model == \"rad\":\n",
    "        P = MigrationModels.radiationModel(origin_pop, destination_pop, S)\n",
    "    elif model == \"gravpow\":\n",
    "        P = MigrationModels.gravityModel(origin_pop, destination_pop, D, alpha, decay=\"power\")\n",
    "    elif model == \"gravexp\":\n",
    "        P = MigrationModels.gravityModel(origin_pop, destination_pop, D, alpha, decay=\"exponential\")\n",
    "    \n",
    "    P = MigrationModels.row_normalize(P)\n",
    "    T_pred = MigrationModels.productionFunction(origin_pop, P, beta=beta)\n",
    "    \n",
    "    return T, T_pred\n",
    "\n",
    "def fit_traditional_models(alpha, args):\n",
    "    T, T_pred = run_traditional_models(alpha, args)\n",
    "    \n",
    "    score = MigrationEvaluationMethods.cpc(T, T_pred)    \n",
    "    return -score\n",
    "    \n",
    "def evaluate_traditional_models(alpha, beta, args):\n",
    "    args[\"beta\"] = beta\n",
    "    T, T_pred = run_traditional_models(alpha, args)\n",
    "    D = args[\"D\"]\n",
    "    \n",
    "    return MigrationEvaluationMethods.evaluate_all(T, T_pred, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrad model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flooded to Unflooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_full_dataset(1, flooded_county_idxs, unflooded_county_idxs, model=\"extrad\")\n",
    "opt_result = scipy.optimize.minimize(fit_traditional_models, x0=[1.0], args=args, bounds=[[0,3]])\n",
    "beta_flooded = opt_result.x[0]\n",
    "alpha = args[\"beta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16908589967733545, 0.13012304380629758)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha, beta_flooded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unflooded to Unflooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11\n",
      "1 11\n",
      "2 11\n",
      "3 11\n",
      "4 11\n",
      "5 11\n",
      "6 11\n",
      "7 11\n",
      "8 11\n",
      "9 11\n",
      "10 11\n"
     ]
    }
   ],
   "source": [
    "alphas = []\n",
    "betas_unflooded = []\n",
    "for i in range(len(years)):\n",
    "    print(i, len(years))\n",
    "    args = get_full_dataset(i, unflooded_county_idxs, unflooded_county_idxs, model=\"extrad\")\n",
    "    opt_result = scipy.optimize.minimize(fit_traditional_models, x0=[1.0], args=args, bounds=[[0,3]])\n",
    "    beta_unflooded = opt_result.x[0]\n",
    "    alpha = args[\"beta\"]\n",
    "    \n",
    "    alphas.append(alpha)\n",
    "    betas_unflooded.append(beta_unflooded)\n",
    "alphas = np.array(alphas)\n",
    "betas_unflooded = np.array(betas_unflooded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03179085324317379"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3535117898490684"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas_unflooded.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"output/extrad_params.txt\",\"w\")\n",
    "f.write(\"alpha,beta_affected,beta_unaffected\\n\")\n",
    "f.write(\"%f,%f,%f\" % (alphas.mean(), beta_flooded, betas_unflooded.mean()))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpc_loss(y_true, y_pred):\n",
    "    return 1.0 - (2.0*K.sum(K.minimum(y_true,y_pred))) / (K.sum(y_true) + K.sum(y_pred))\n",
    "\n",
    "def baseline_model():\n",
    "    inputs = Input(shape=(4,))\n",
    "    x = inputs\n",
    "\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    \n",
    "    outputs = Dense(1, activation=\"relu\")(x)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    optimizer = Adam(lr=0.001)\n",
    "\n",
    "    model.compile(loss=cpc_loss, metrics=[\"mse\", cpc_loss], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flooded to Unflooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train\n",
    "args = get_full_dataset(1, flooded_county_idxs, unflooded_county_idxs, model=\"dl\")\n",
    "beta = args[\"beta\"]\n",
    "x_train, y_train = get_pairs_from_full_dataset(args)\n",
    "    \n",
    "y_train_binary = y_train.copy().astype(int)\n",
    "mask = y_train>0\n",
    "\n",
    "positive_indices = np.where(mask)[0]\n",
    "negative_indices = np.where(~mask)[0]\n",
    "\n",
    "num_positive = positive_indices.shape[0]\n",
    "num_negative = negative_indices.shape[0]\n",
    "new_num_negative = max(negative_indices.shape[0], 40*num_positive)\n",
    "\n",
    "negative_indices = np.random.choice(negative_indices, size=new_num_negative, replace=True)\n",
    "\n",
    "new_indices = np.concatenate([\n",
    "    positive_indices,\n",
    "    negative_indices\n",
    "])\n",
    "\n",
    "x_train = x_train[new_indices]\n",
    "y_train = y_train[new_indices]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "    \n",
    "    \n",
    "K.clear_session()\n",
    "early_stopping = EarlyStopping(monitor=\"cpc_loss\", patience=100, restore_best_weights=True)\n",
    "model = keras.wrappers.scikit_learn.KerasRegressor(\n",
    "    build_fn=baseline_model,\n",
    "    epochs=500,\n",
    "    batch_size=2**12,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "history = model.fit(x_train, y_train)\n",
    "\n",
    "model.model.save(\"output/dl_flooded.h5\")\n",
    "joblib.dump(scaler, \"output/scaler_flooded.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train\n",
    "args = get_full_dataset(2, unflooded_county_idxs, unflooded_county_idxs, model=\"dl\")\n",
    "beta = args[\"beta\"]\n",
    "x_train, y_train = get_pairs_from_full_dataset(args)\n",
    "    \n",
    "y_train_binary = y_train.copy().astype(int)\n",
    "mask = y_train>0\n",
    "\n",
    "positive_indices = np.where(mask)[0]\n",
    "negative_indices = np.where(~mask)[0]\n",
    "\n",
    "num_positive = positive_indices.shape[0]\n",
    "num_negative = negative_indices.shape[0]\n",
    "new_num_negative = max(negative_indices.shape[0], 40*num_positive)\n",
    "\n",
    "negative_indices = np.random.choice(negative_indices, size=new_num_negative, replace=True)\n",
    "\n",
    "new_indices = np.concatenate([\n",
    "    positive_indices,\n",
    "    negative_indices\n",
    "])\n",
    "\n",
    "x_train = x_train[new_indices]\n",
    "y_train = y_train[new_indices]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "    \n",
    "    \n",
    "K.clear_session()\n",
    "early_stopping = EarlyStopping(monitor=\"cpcLoss\", patience=100, restore_best_weights=True)\n",
    "model = keras.wrappers.scikit_learn.KerasRegressor(\n",
    "    build_fn=baseline_model,\n",
    "    epochs=500,\n",
    "    batch_size=2**12,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "history = model.fit(x_train, y_train)\n",
    "\n",
    "model.model.save(\"output/dl_unflooded.h5\")\n",
    "joblib.dump(scaler, \"output/scaler_unflooded.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save(\"output/dl_unflooded.h5\")\n",
    "joblib.dump(scaler, \"output/scaler_unflooded.p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
